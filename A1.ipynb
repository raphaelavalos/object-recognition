{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"A1.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["wq0Z8XLZLQTk","CQz1DjNYWErP","P1bbB04iSSVe","156S_udKTCm-","5DwgvwPXTzao","IoSIB18cV0cX","mfk9XmtdGPZG","_Bzv1Udh22Lm"]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"VMXQmPYx5zsL","colab_type":"text"},"cell_type":"markdown","source":["<h1><big><center>Object recognition and computer vision 2018/2019</center></big></h1>\n","\n","<h3><big><center><a href=\"http://www.di.ens.fr/~ponce/\">Jean Ponce</a>, <a href=\"http://www.di.ens.fr/~laptev/\">Ivan Laptev</a>, <a href=\"http://lear.inrialpes.fr/~schmid/\">Cordelia Schmid</a> and <a href=\"http://www.di.ens.fr/~josef/\">Josef Sivic</a></center></big></h3>\n","\n","\n","<h2><big><center> Assignment 1: Instance-level recognition</center></big></h2>\n","\n","<h5><big><center>Adapted from practicals from <a href=\"http://www.robots.ox.ac.uk/~vgg/practicals/overview/index.html\">Andrea Vedaldi and Andrew Zisserman</a> \n","  <br>by <a href=\"https://www.di.ens.fr/~varol/\">Gul Varol</a> and <a href=\"https://www.di.ens.fr/~iroccosp/\">Ignacio Rocco</a></center></big></h5>\n","\n","</br>\n","<p align=\"center\">\n","<img height=300px src=\"http://www.di.ens.fr/willow/teaching/recvis17/assignment1/images/image12.png\"/></p>\n","<p align=\"center\"></p>"]},{"metadata":{"id":"JQ3NoPjbP6ur","colab_type":"text"},"cell_type":"markdown","source":["**STUDENT**:  YOUR NAME HERE\n","\n","**EMAIL**:  YOUR EMAIL HERE"]},{"metadata":{"id":"Uo8bK9UHusNO","colab_type":"text"},"cell_type":"markdown","source":["# Guidelines\n","\n","The purpose of this assignment is that you get hands-on experience with the topics covered in class, which will help you understand these topics better. Therefore, ** it is imperative that you do this assignment yourself. No code sharing will be tolerated. **\n","\n","Once you have completed the assignment, you will submit the `ipynb` file containing **both** code and results. For this, make sure to **run your notebook completely before submitting**.\n","\n","The `ipynb` must be named using the following format: **A1_LASTNAME_Firstname.ipynb**, and submitted in the **class Moodle page**."]},{"metadata":{"id":"w2wRNvWF6Qu2","colab_type":"text"},"cell_type":"markdown","source":["# Goal\n","The goal of instance-level recognition is to match (recognize) a specific object or scene.  Examples include recognizing a specific building, such as Notre Dame, or a specific painting, such as `Starry Nightâ€™ by Van Gogh. The object is recognized despite changes in scale, camera viewpoint, illumination conditions and partial occlusion. An important application is image retrieval - starting from an image of an object of interest (the query), search  through an image dataset to obtain (or retrieve) those images that contain the target object.\n","\n","The goal of this assignment is to experiment and get basic practical experience with the methods that enable specific object recognition. It includes: (i) using SIFT features to obtain sparse matches between two images; (ii) using similarity co-variant detectors to cover changes in viewpoint; (iii) vector quantizing the SIFT descriptors into visual words to enable large scale retrieval; and (iv) constructing and using an image retrieval system to identify objects."]},{"metadata":{"id":"bpW3I6a66Sji","colab_type":"text"},"cell_type":"markdown","source":["# Setup environment"]},{"metadata":{"id":"yTLfSBZe9QQE","colab_type":"text"},"cell_type":"markdown","source":["## Download and install CyVLFeat"]},{"metadata":{"id":"fCDiN_JPo1vD","colab_type":"code","colab":{}},"cell_type":"code","source":["!wget -N http://www.di.ens.fr/willow/teaching/recvis/assignment1/install_cyvlfeat.py\n","%run install_cyvlfeat.py"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kEcxIyN3O_ME","colab_type":"text"},"cell_type":"markdown","source":["## Imports"]},{"metadata":{"id":"GFGh7fvKZyKT","colab_type":"code","colab":{}},"cell_type":"code","source":["import cyvlfeat\n","import numpy as np\n","from scipy.misc import imread,imresize,imrotate\n","from urllib.request import urlopen\n","import matplotlib as mpl\n","import matplotlib.pyplot as plt\n","import warnings\n","from cyvlfeat.plot import plotframes\n","from scipy.io import loadmat\n","import numpy as np\n","\n","# change some default matplotlib parameters\n","mpl.rcParams['axes.grid'] = False\n","mpl.rcParams['figure.dpi'] = 120\n","\n","# ignore warnings\n","warnings.filterwarnings('ignore')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wzE1SHZwPCF1","colab_type":"text"},"cell_type":"markdown","source":["## Download and uncompress data"]},{"metadata":{"id":"lc-NOqeeAAzN","colab_type":"code","colab":{}},"cell_type":"code","source":["print('downloading assignment images...')\n","!wget -c http://www.di.ens.fr/willow/teaching/recvis/assignment1/A1_images.tar.gz\n","print('done!')\n","print('uncompressing...')\n","!tar -xzf A1_images.tar.gz\n","print('done!')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"SD0WPt9P7kor","colab_type":"text"},"cell_type":"markdown","source":["# Part 1: Sparse features for matching specific objects in images"]},{"metadata":{"id":"Hvi7css7PFQW","colab_type":"text"},"cell_type":"markdown","source":["## Feature point detection\n","\n","The SIFT feature has both a *detector* and a *descriptor*. The *detector* used in SIFT corresponds to the \"difference of Gaussians\" (DoG) detector, which is an approximation of the \"Laplacian of Gaussian\"  (LoG) detector.\n"," \n","We will start by computing and visualizing the SIFT feature detections (usually called frames) for two images of the same object (a building facade). Load an image, rotate and scale it, and then display the original and transformed pair:"]},{"metadata":{"id":"U1odHjjFDpjf","colab_type":"code","colab":{}},"cell_type":"code","source":["# Load an image\n","im1 = imread('data/oxbuild_lite/all_souls_000002.jpg')\n","\n","# Let the second image be a rotated and scaled version of the first\n","im1prime = imresize(imrotate(np.pad(im1,((0,0),(200,200),(0,0)),'constant'),35,'bilinear'),0.7)\n","\n","f, (ax1, ax2) = plt.subplots(1, 2)\n","ax1.imshow(im1)\n","ax2.imshow(im1prime)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"kLHQhVWDGpeT","colab_type":"text"},"cell_type":"markdown","source":["A SIFT frame is a circle with an orientation and is specified by four parameters: the center $x$, $y$, the scale $s$, and the rotation $\\theta$ (in radians), resulting in a vector of four parameters $(x, y, s, \\theta)$.\n","\n","Now compute and visualise the SIFT feature detections (frames):"]},{"metadata":{"id":"yj1q_hagYQmp","colab_type":"code","colab":{}},"cell_type":"code","source":["def rgb2gray(rgb):\n","    return np.float32(np.dot(rgb[...,:3], [0.299, 0.587, 0.114])/255.0)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"L6NN3Irxh37x","colab_type":"code","colab":{}},"cell_type":"code","source":["# Compute SIFT features for each\n","\n","[frames1, descrs1] = cyvlfeat.sift.sift(rgb2gray(im1),peak_thresh=0.01)\n","\n","[frames1prime, descrs1prime] = cyvlfeat.sift.sift(rgb2gray(im1prime),peak_thresh=0.01)\n","\n","f, (ax1, ax2) = plt.subplots(1, 2)\n","plt.sca(ax1)\n","plt.imshow(im1)\n","plotframes(frames1,linewidth=1)\n","\n","plt.sca(ax2)\n","plt.imshow(im1prime)\n","plotframes(frames1prime,linewidth=1)\n","\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"J7EE_bxoKlPG","colab_type":"text"},"cell_type":"markdown","source":["Examine the second image and its rotated and scaled version and convince yourself that the detections overlap the same scene regions (even though the circles' positions have moved in the image and their radius' have changed). \n","This demonstrates that the detection process (is co-variant or equi-variant) with translations, rotations and isotropic scalings. This class of transformations is known as a similarity or equiform."]},{"metadata":{"id":"wq0Z8XLZLQTk","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.1 ::\n","\n","Now repeat the exercise with a pair of natural images. \n","\n","Start by loading the second one: `data/oxbuild_lite/all_souls_000015.jpg`\n","\n","Plot the images and feature frames. Again you should see that many of the detections overlap the same scene region. Note that, while repeatability occurs for the pair of natural views, it is much better for the synthetically rotated pair."]},{"metadata":{"id":"f5ZSA0olLwHK","colab_type":"code","colab":{}},"cell_type":"code","source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"TxipHsrTMdxC","colab_type":"text"},"cell_type":"markdown","source":["The number of detected features can be controlled by changing the peakThreshold option. A larger value will select features that correspond to higher contrast structures in the image. "]},{"metadata":{"id":"auefGnqTMfkj","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.2 ::\n","\n","***For the same image, produce 3 sub-figures with different values of peakThreshold. Comment.***"]},{"metadata":{"id":"HKYdfbM-Mfvq","colab_type":"code","colab":{}},"cell_type":"code","source":["##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Vz-WtILYjnVR","colab_type":"text"},"cell_type":"markdown","source":["######################\n","\n","WRITE YOUR ANSWER HERE\n","\n","######################"]},{"metadata":{"id":"LpSJygkrMEOc","colab_type":"text"},"cell_type":"markdown","source":["## Feature point description and matching "]},{"metadata":{"id":"CQz1DjNYWErP","colab_type":"text"},"cell_type":"markdown","source":["### Introduction to feature descriptors\n","\n","\n","The parameters $(t_x, t_y, s, \\theta)$ of the detected *frames*,  can be used to extract a **scaled and oriented RGB patch** around $(t_x,t_y)$, used to *describe* the feature point.\n","\n","The simplest possible descriptor would be to (i) resize these patches to a common size (eg. 30x30) and (ii) flatten to a vector. However, in practice we use more sophisticated descriptors such as SIFT, that is based on histograms of gradient orientations."]},{"metadata":{"id":"P1bbB04iSSVe","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.3 ::\n","\n","\n","***What is the interest of using SIFT descriptors over these flattened RGB patches?***"]},{"metadata":{"id":"jPAkIV3TSZun","colab_type":"text"},"cell_type":"markdown","source":["######################\n","\n","   WRITE YOUR ANSWER HERE  \n","\n","######################\n"]},{"metadata":{"id":"156S_udKTCm-","colab_type":"text"},"cell_type":"markdown","source":["### Descriptor matching\n","SIFT descriptors are 128-dimensional vectors, and can be directly *matched* to find correspondences between images. We will start with the simplest matching scheme (first nearest neighbour of descriptors in terms of Euclidean distance) and then add more sophisticated methods to eliminate any mismatches."]},{"metadata":{"id":"5DwgvwPXTzao","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.4 ::\n","\n","\n","For each descriptor in im1, assign a matching descriptor in im2 by picking its first nearest neighbour.\n","\n","Populate the second column of the matches vector."]},{"metadata":{"id":"bn3PCcjZwji-","colab_type":"code","colab":{}},"cell_type":"code","source":["# number of detections in image 1\n","N_frames1 = frames1.shape[0]\n","# allocate matrix for storing the matches\n","matches=np.zeros((N_frames1,2),dtype=np.int)\n","# the first column of the matrix are the indices on image 1: 0,1,2,....,N_frames1-1\n","matches[:,0]=range(N_frames1)\n","\n","# write code to find the matches in image 2 of each feature in image 1 \n","# populate the second column of the matches vector\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"7hWjq9G2aHdh","colab_type":"code","colab":{}},"cell_type":"code","source":["# plot matches\n","plt.imshow(np.concatenate((im1,im2),axis=1))\n","for i in range(N_frames1):\n","    j=matches[i,1]\n","    # plot dots at feature positions\n","    plt.gca().scatter([frames1[i,0],im1.shape[1]+frames2[j,0]], [frames1[i,1],frames2[j,1]], s=5, c=[0,1,0])\n","    # plot lines\n","    plt.plot([frames1[i,0],im1.shape[1]+frames2[j,0]],[frames1[i,1],frames2[j,1]],linewidth=0.5)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VdnoQ3gcV9mK","colab_type":"text"},"cell_type":"markdown","source":["## Improving SIFT matching (i) using Loweâ€™s second nearest neighbour test\n"]},{"metadata":{"id":"84MPzJZpWkVO","colab_type":"text"},"cell_type":"markdown","source":["Lowe introduced a second nearest neighbour (2nd NN) test to identify, and hence remove, ambiguous matches. The idea is to identify distinctive matches by a threshold on the ratio of first to second NN distances.\n","\n","The ratio is:  $$NN_{ratio} = \\frac{1^{st}\\text{NN distance}}{2^{nd}\\text{NN distance}}.$$\n","\n"]},{"metadata":{"id":"IoSIB18cV0cX","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.5 ::\n","\n","***For each descriptor in im1, compute the ratio between the first and second nearest neighbour distances.***\n","\n","***Populate the ratio vector.***\n"]},{"metadata":{"id":"BOo3--vjXiiY","colab_type":"code","colab":{}},"cell_type":"code","source":["# allocate matrix for storing the matches\n","ratio=np.zeros((N_frames1,1))\n","\n","# write code to find the 1st/2nd NN ratio for each descriptor in im1\n","# populate the ratio vector\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"f3gKZDeIYE_X","colab_type":"text"},"cell_type":"markdown","source":["The ratio vector will be now used to retain only the matches that are above a given threshold. \n","\n","A value of $NN_{threshold} = 0.8$ is often a good compromise between losing too many matches and rejecting mismatches."]},{"metadata":{"id":"I9_OvKEcZEGG","colab_type":"code","colab":{}},"cell_type":"code","source":["NN_threshold=0.8\n","\n","filtered_indices = np.flatnonzero(ratio<NN_threshold)\n","filtered_matches = matches[filtered_indices,:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"paGZYlI7YDXn","colab_type":"code","colab":{}},"cell_type":"code","source":["# plot matches\n","plt.imshow(np.concatenate((im1,im2),axis=1))\n","for idx in range(filtered_matches.shape[0]):\n","    i=filtered_matches[idx,0]\n","    j=filtered_matches[idx,1]\n","    # plot dots at feature positions\n","    plt.gca().scatter([frames1[i,0],im1.shape[1]+frames2[j,0]], [frames1[i,1],frames2[j,1]], s=5, c=[0,1,0]) \n","    # plot lines\n","    plt.plot([frames1[i,0],im1.shape[1]+frames2[j,0]],[frames1[i,1],frames2[j,1]],linewidth=0.5)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ITytjPQ6FhHZ","colab_type":"text"},"cell_type":"markdown","source":["## Improving SIFT matching (ii) by geometric verification"]},{"metadata":{"id":"PsEEzjgVFshn","colab_type":"text"},"cell_type":"markdown","source":["In addition to the 2nd NN test, we can also require consistency between the matches and a geometric transformation between the images. For the moment we will look for matches that are consistent with a similarity transformation:\n","\n","$$\\begin{bmatrix} x_2 \\\\ y_2 \\end{bmatrix} = \n","sR(\\theta) \\begin{bmatrix} x_1 \\\\ y_1 \\end{bmatrix} + \\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} $$\n","\n","which consists of a rotation by $\\theta$, an isotropic scaling (i.e. same in all directions) by s, and a translation by a vector $(t_x, t_y)$. This transformation is specified by four parameters $(s,\\theta,t_x,t_y)$ and can be computed from a single correspondence between SIFT detections in each image."]},{"metadata":{"id":"mfk9XmtdGPZG","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.6 ::\n","\n","Given a detected feature with parameters $(x_1, y_1, s_1, \\theta_1)$ in image $1$ matching a feature $(x_2, y_2, s_2, \\theta_2)$ in image $2$, work out how to find out the parameters $(t_x,t_y,s,\\theta)$ of the transformation mapping points from image $1$ to image $2$.\n","\n"]},{"metadata":{"id":"VF3nO5rX0IEX","colab_type":"text"},"cell_type":"markdown","source":["\n","$$\\theta=\\text{COMPLETE HERE}$$\n","$$s=\\text{COMPLETE HERE}$$\n","$$\\begin{bmatrix} t_x \\\\ t_y \\end{bmatrix} =\\text{COMPLETE HERE}$$\n"]},{"metadata":{"id":"56tVW8dt3t31","colab_type":"text"},"cell_type":"markdown","source":["The matches consistent with a similarity can then be found using the RANSAC algorithm, described by the following steps:\n","\n","For each tentative correspondence in turn:\n","\n","* compute the similarity transformation;\n","* map all the SIFT detections in one image to the other using this transformation;\n","* accept matches that are within a threshold distance to the mapped detection (inliers);\n","* count the number of accepted matches;\n","* optionally, fit a more accurate affine transformation or homography to the accepted matches and test re-validate the matches.\n","\n","Finally, choose the transformation with the highest count of inliers\n","\n","After this algorithm the inliers are consistent with the transformation and are retained, and most mismatches should now be removed."]},{"metadata":{"id":"_Bzv1Udh22Lm","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 1.7 ::"]},{"metadata":{"id":"vaoT7lbV2xVF","colab_type":"code","colab":{}},"cell_type":"code","source":["def ransac(frames1,frames2,matches,N_iters=100,dist_thresh=15):\n","  # initialize\n","  max_inliers=0\n","  tnf=None\n","  # run random sampling\n","  for it in range(N_iters):\n","      # pick a random sample\n","      i = np.random.randint(0,frames1.shape[0])\n","      x_1,y_1,s_1,theta_1=frames1[i,:]\n","      j = matches[i,1]\n","      x_2,y_2,s_2,theta_2=frames2[j,:]\n","      \n","      # estimate transformation\n","      \n","      # COMPLETE BELOW #\n","      \n","      # theta = \n","      # s = \n","      # t_x =\n","      # t_y = \n","      \n","      # evaluate estimated transformation\n","      X_1 = frames1[:,0]\n","      Y_1 = frames1[:,1]\n","      X_2 = frames2[matches[:,1],0]\n","      Y_2 = frames2[matches[:,1],1]\n","      \n","      X_1_prime = s*(X_1*np.cos(theta)-Y_1*np.sin(theta))+t_x\n","      Y_1_prime = s*(X_1*np.sin(theta)+Y_1*np.cos(theta))+t_y\n","      \n","      dist = np.sqrt((X_1_prime-X_2)**2+(Y_1_prime-Y_2)**2)\n","      \n","      inliers_indices = np.flatnonzero(dist<dist_thresh)\n","      num_of_inliers = len(inliers_indices)\n","      \n","      # keep if best\n","      if num_of_inliers>max_inliers:\n","        max_inliers=num_of_inliers\n","        best_inliers_indices = inliers_indices\n","        tnf = [t_x,t_y,s,theta]\n","        \n","  return (tnf,best_inliers_indices)\n","      "],"execution_count":0,"outputs":[]},{"metadata":{"id":"M8ZpDU_1K5Md","colab_type":"code","colab":{}},"cell_type":"code","source":["tnf,inliers_indices=ransac(frames1,frames2,matches)\n","filtered_matches = matches[inliers_indices,:]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"99CjeXs6Ll1c","colab_type":"code","colab":{}},"cell_type":"code","source":["# plot matches filtered with RANSAC\n","plt.imshow(np.concatenate((im1,im2),axis=1))\n","for idx in range(filtered_matches.shape[0]):\n","    i=filtered_matches[idx,0]\n","    j=filtered_matches[idx,1]\n","    # plot dots at feature positions\n","    plt.gca().scatter([frames1[i,0],im1.shape[1]+frames2[j,0]], [frames1[i,1],frames2[j,1]], s=5, c=[0,1,0]) \n","    # plot lines\n","    plt.plot([frames1[i,0],im1.shape[1]+frames2[j,0]],[frames1[i,1],frames2[j,1]],linewidth=0.5)\n","plt.show()"],"execution_count":0,"outputs":[]},{"metadata":{"id":"vXz0ETuhP7vE","colab_type":"text"},"cell_type":"markdown","source":["# Part 2: Compact descriptors for image retrieval"]},{"metadata":{"id":"-7rqcpLhP_XJ","colab_type":"text"},"cell_type":"markdown","source":["In large scale retrieval the goal is to match a query image to a large database of images (for example the WWW or Wikipedia). \n","\n","The quality of an image match is measured as the number of geometrically verified feature correspondences between the query and a database image. While the techniques discussed in Part 1 are sufficient to do this, in practice they require too much memory to store the SIFT descriptors for all the detections in all the database images.\n","\n","In this part we will see how we can compute a *global* image descriptor from the set of SIFT descriptors using the bag-of-visual-words (BoVW) approach.\n","\n","Then, we will see how these global descriptors can be used to rapidly retrieve a shortlist of candidate database images given a query image. Finally, we will see how to re-rank the shortlist of candidates using a geometric verification technique that requires only the *detector frames* and their assigned visual word indices; remember the SIFT descriptors are only used to compute the compact BoVW descriptors and then discarded.\n"]},{"metadata":{"id":"mo0M1nsLMKQz","colab_type":"text"},"cell_type":"markdown","source":["## Download preprocessed dataset of paintings"]},{"metadata":{"id":"lfys5RNKa5aP","colab_type":"code","colab":{}},"cell_type":"code","source":["print('downloading dictionary of SIFT features and precomputed BoVW descriptors for painting dataset...')\n","!wget -c http://www.di.ens.fr/willow/teaching/recvis/assignment1/paintings_imdb_SIFT_10k_preprocessed.mat\n","print('done!')"],"execution_count":0,"outputs":[]},{"metadata":{"id":"ZgXUBMLkMYKR","colab_type":"text"},"cell_type":"markdown","source":["## Load preprocessed dataset of paintings"]},{"metadata":{"id":"0TtX0gzHMg6y","colab_type":"text"},"cell_type":"markdown","source":["We will now load the preprocessed dataset of paintings. The construction of this dataset has involved several steps.\n","\n","\n","1.   SIFT features were extracted from all painting in the dataset\n","2.   A global vocabulary of SIFT descriptors was computed using K-means clustering. These are the visual words of our dataset.\n","3.  The SIFT features for each painting were assigned to the nearest word, and a compact descriptor was generated for each painting. This compact descriptor consists in the normalized histogram of words. The histogram normalization itself involves 3 different steps:\n","    - a) TF-IDF weighting: each word is re-weighted according to its TF-IDF value. This removes weights to words that are very common and therefore not too descriptive.\n","    - b) Square-rooting: each element is square-rooted\n","    - c) L2-normalization: The whole histogram is L2-normalized.\n","    \n","\n","\n"]},{"metadata":{"id":"vyQ8Ts0mUSEr","colab_type":"code","colab":{}},"cell_type":"code","source":["imdb=loadmat('paintings_imdb_SIFT_10k_preprocessed.mat')\n","\n","feature_vocab=np.transpose(imdb['vocab'])\n","imdb_hists=imdb['index']\n","imdb_tfidf=imdb['idf']\n","\n","imdb_url = lambda idx: imdb['images'][0][0][3][0][idx].item()\n","\n","num_words = feature_vocab.shape[0]\n","num_paintings = imdb_hists.shape[1]\n","\n","print('The vocabulary of SIFT features contains %s  visual words' % num_words)\n","print('The dictionary index contains %s histograms corresponding to each painting in the dataset' % num_paintings)\n","print('The tdf-idf vector has shape '+str(imdb_tfidf.shape))"],"execution_count":0,"outputs":[]},{"metadata":{"id":"hY3JQk0FB_gg","colab_type":"code","colab":{}},"cell_type":"code","source":["painting = imread('data/queries/mistery-painting1.jpg')\n","\n","[frames, descrs] = cyvlfeat.sift.sift(rgb2gray(painting), peak_thresh=0.01)\n","\n","plt.imshow(painting)\n","plotframes(frames,linewidth=1)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"jHznDmPx8VzY","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 2.1 ::\n","\n","Construct a KDTree of the vocabulary for fast NN search. Then use the KDTree to find the closest word in the vocabulary to each descriptor of the query image.\n"]},{"metadata":{"id":"Lg79y_1l8xIp","colab_type":"code","colab":{}},"cell_type":"code","source":["from sklearn.neighbors import KDTree\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"RubSI5nU9C8j","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 2.2 ::\n","Compute the compact BoVW descriptor of the query image."]},{"metadata":{"id":"BOsAtuQYPS5R","colab_type":"code","colab":{}},"cell_type":"code","source":["query_hist=np.zeros((num_words,1))\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"FD-ZaEXGGXDu","colab_type":"code","colab":{}},"cell_type":"code","source":["# process histogram\n","query_hist = query_hist*imdb_tfidf\n","query_hist = np.sqrt(query_hist)\n","query_hist = query_hist/np.linalg.norm(query_hist)\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"r3ylUZ_H9P0v","colab_type":"text"},"cell_type":"markdown","source":["### :: TASK 2.3 ::\n","\n","Compute the matching score with each image from the database.\n","\n"]},{"metadata":{"id":"Ls8i7Zd6-M7P","colab_type":"code","colab":{}},"cell_type":"code","source":["scores = np.zeros((1,num_paintings))\n","\n","##########################\n","#                        #\n","#  WRITE YOUR CODE HERE  #\n","#                        #\n","##########################"],"execution_count":0,"outputs":[]},{"metadata":{"id":"tTbQ-MJjUvkH","colab_type":"code","colab":{}},"cell_type":"code","source":["# sort in descending order\n","scores_sorted_idx = np.argsort(-scores)\n","scores_sorted = scores.ravel()[scores_sorted_idx]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1EdES34d1Gxf","colab_type":"code","colab":{}},"cell_type":"code","source":["# plot top matches\n","N=10\n","top_N_idx = scores_sorted_idx.ravel()[:N]\n","\n","for i in range(N):\n","  # download images\n","  url = imdb_url(top_N_idx[i])      \n","  with urlopen(url) as file:\n","      img = imread(file, mode='RGB')\n","  # choose subplot\n","  plt.subplot(int(np.ceil(N/5)),5,i+1)\n","  # plot\n","  plt.imshow(img)\n","  plt.axis('off')\n","  plt.title('score %1.2f' % scores_sorted.ravel()[i])\n","  "],"execution_count":0,"outputs":[]},{"metadata":{"id":"Im_0QKwzQZj5","colab_type":"text"},"cell_type":"markdown","source":["## AUTHORSHIP STATEMENT\n","\n","I declare that the preceding work was the sole result of my own effort and that I have not used any code or results from third-parties.\n","\n","FILL IN YOUR NAME HERE"]}]}